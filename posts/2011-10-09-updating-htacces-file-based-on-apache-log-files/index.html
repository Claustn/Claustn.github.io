<!doctype html><html lang=en><head><title>Updating .Htacces file based on Apache log files :: XenoBlog — Random rants</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="I am still seeing massive amounts of referal traffic hitting my site, eating up my bandwidth.. I did not get time to update my .htaccess file for the last 2 days.. and within the last 24 hours I have had more than 6000 hits, generating in almost 24.000 pageviews&amp;hellip; Generating more than 1 GB worth of traffic (So at that speed I will reach my 10 GB limit soon)
Looking through the Apache logs, figuring out which sites I get most referral traffic from, getting the hostnames, transforming them into a format that can be used by the Apache rewrite engine in the ."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/posts/2011-10-09-updating-htacces-file-based-on-apache-log-files/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/assets/green.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon/green.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Updating .Htacces file based on Apache log files :: XenoBlog"><meta property="og:description" content="I am still seeing massive amounts of referal traffic hitting my site, eating up my bandwidth.. I did not get time to update my .htaccess file for the last 2 days.. and within the last 24 hours I have had more than 6000 hits, generating in almost 24.000 pageviews&amp;hellip; Generating more than 1 GB worth of traffic (So at that speed I will reach my 10 GB limit soon)
Looking through the Apache logs, figuring out which sites I get most referral traffic from, getting the hostnames, transforming them into a format that can be used by the Apache rewrite engine in the ."><meta property="og:url" content="/posts/2011-10-09-updating-htacces-file-based-on-apache-log-files/"><meta property="og:site_name" content="Updating .Htacces file based on Apache log files"><meta property="og:image" content="/img/favicon/green.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:section" content="Everyday"><meta property="article:published_time" content="2011-10-09 20:12:12 +0000 UTC"></head><body><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>XenoBlog</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=https://github.com/claustn>github</a></li><li><a href=/index.xml>RSS</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=https://github.com/claustn>github</a></li><li><a href=/index.xml>RSS</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=/posts/2011-10-09-updating-htacces-file-based-on-apache-log-files/>Updating .Htacces file based on Apache log files</a></h1><div class=post-meta><span class=post-date>2011-10-09</span></div><div class=post-content><div><p>I am still seeing massive amounts of referal traffic hitting my site, eating up my bandwidth.. I did not get time to update my .htaccess file for the last 2 days.. and within the last 24 hours I have had more than 6000 hits, generating in almost 24.000 pageviews&mldr; Generating more than 1 GB worth of traffic (So at that speed I will reach my 10 GB limit soon)</p><p>Looking through the Apache logs, figuring out which sites I get most referral traffic from, getting the hostnames, transforming them into a format that can be used by the Apache rewrite engine in the .htaccess file has been time consuming. So I decided that some powershell magic, might speed up the process a bit.</p><p>[ps]<br>function Select-FileDialog<br>{<br>param(<br>[string]$Title,<br>[string]$Directory,<br>[string]$Filter="All Files (*.*)|*.*")<br>[System.Reflection.Assembly]::LoadWithPartialName(&ldquo;System.Windows.Forms&rdquo;) | Out-Null<br>$objForm = New-Object System.Windows.Forms.OpenFileDialog<br>$objForm.InitialDirectory = $Directory<br>$objForm.Filter = $Filter<br>$objForm.Title = $Title<br>$Show = $objForm.ShowDialog()<br>If ($Show -eq &ldquo;OK&rdquo;)<br>{<br>Return $objForm.FileName<br>}<br>Else<br>{<br>Write-Error &ldquo;Operation cancelled by user.&rdquo;<br>}<br>}</p><p>#Function to create the http rewrite rules.</p><p>Function Create-Rewrite {<br>Param (<br>$Hostname<br>)</p><p>$HtaRule = &ldquo;RewriteCond %{HTTP_REFERER} ^http://&rdquo; + &ldquo;$($hostname.replace(".","."))&rdquo; +&rdquo; [OR]&rdquo;<br>$script:BlockList += $HtaRule<br>}</p><p>Function add-htaccess {<br>Param (<br>$HtaRules<br>)<br>(Get-Content $htaccess) | foreach-object {<br>$_<br>if ($_ -match &ldquo;RewriteEngine&rdquo;) {<br>if (!(Select-String -simplematch &ldquo;$htarules&rdquo; -Path $htaccess))<br>{<br>$HtaRules<br>}<br>}</p><p>} | set-Content $tempFile<br>Copy-Item $tempFile $htaccess<br>}</p><p>Function Upload-Ftp {<br>Param ([Parameter(Position=0, Mandatory=$true)]<br>[ValidateNotNullOrEmpty()]<br>[System.String]<br>$FTPHost,<br>[Parameter(Position=1)]<br>[ValidateNotNull()]<br>$File<br>)<br>$webclient = New-Object System.Net.WebClient<br>$uri = New-Object System.Uri($ftphost)</p><p>&ldquo;Uploading $File&mldr;&rdquo;</p><p>$webclient.UploadFile($uri, $File)<br>}</p><p>#Variables<br>$log = Select-FileDialog -Title &ldquo;Select an Apache logfile&rdquo;<br>$htaccess = &ldquo;c:\Temp.htaccess&rdquo;<br>$tempFile = [IO.Path]::GetTempFileName()<br>$URLCount = 15<br>$FTPUsername = &ldquo;Username&rdquo;<br>$FTPPassword = &ldquo;PassW0rd&rdquo;</p><p>$BlockList = "&rdquo;<br>#Create list of sites to block<br>$script:BlockList = @()</p><p>#Get the list of URLS in the the logfile, capturing each element into different named capturing groups</p><p>$urls = Select-String &lsquo;^(?&lt;client>\S+)\s+(?&lt;auth>\S+\s+\S+)\s+[(?&lt;datetime>[^]]+)]\s+&rdquo;(?:GET|POST|HEAD) (?&lt;file>[^ ?"]+)??(?&lt;parameters>[^ ?"]+)? HTTP/[0-9.]+&rdquo;\s+(?&lt;status>[0-9]+)\s+(?&lt;size>[-0-9]+)\s+&rdquo;(?&lt;referrer>[^&rdquo;]*)"\s+&rdquo;(?&lt;useragent>[^&rdquo;]*)&ldquo;$&rsquo; $log |<br>Select -Expand Matches | Foreach { $_.Groups[&ldquo;referrer&rdquo;].value }</p><p>#Output statistics for the referer hostnames (Only show top 15)<br>$urls | group | ForEach -begin { $total = 0 } `<br>-process { $total += $_.Count; $_ } |Sort Count | Select Count, Name |<br>Add-Member ScriptProperty Percent { &ldquo;{0,15:0.00}%&rdquo; -f (100*$this.Count/$Total) } -Passthru | select -Last $URLCount</p><p>#Getting the base hostnames from the complete URLS, and outputs statistics to the screen.</p><p>$hosts = $urls | Select-String &lsquo;\b[a-z][a-z0-9+-.]*://([a-z0-9-._~%!$&()*+,;=]+@)?(?&lt;host>[a-z0-9-._~%]+|[[a-z0-9-._~%!$&()*+,;=:]+])&rsquo; |<br>Select -Expand Matches | Foreach { $_.Groups[&ldquo;host&rdquo;].value } | group | sort count | where {($_.name -notlike &ldquo;*xipher.dk*") -and ($_.Count -gt 100)} |<br>ForEach -begin { $total = 0 } `<br>-process { $total += $_.Count; $_ } | Sort Count | Select Count, Name |<br>Add-Member ScriptProperty Percent { &ldquo;{0,10:0.00}%&rdquo; -f (100*$this.Count/$Total) } -Passthru</p><p>Write-Host &ldquo;List of root hostnames&rdquo;</p><p>$hosts</p><p>Foreach ($Url in $hosts) {</p><p>Create-Rewrite $url.Name<br>}</p><p>Foreach ($Block in $script:BlockList) {<br>add-htaccess $Block<br>}</p><p>notepad $htaccess</p><p>$script:BlockList</p><p>Upload-Ftp -FTPHost &ldquo;ftp://$($FTPUsername):$($FTPPassword)@xipher.dk/httpdocs/.htaccess&rdquo; -File $htaccess<br>Upload-Ftp -FTPHost &ldquo;ftp://$($FTPUsername):$($FTPPassword)@xipher.dk/httpdocs/WordPress/.htaccess&rdquo; -File $htaccess<br>[/ps]</p><p>Unfortunately my current hosting company, does not allow me to download the log files via FTP, but I have to connect to the Parallels interface and download it manually.. (I have not had the time looking into automating this part yet, so this is still a manual step)<br>That is why I added a little function to use a GUI to pick the access_log file.</p><p>[ps]<br>function Select-FileDialog<br>{<br>param(<br>[string]$Title,<br>[string]$Directory,<br>[string]$Filter="All Files (*.*)|*.*")<br>[System.Reflection.Assembly]::LoadWithPartialName(&ldquo;System.Windows.Forms&rdquo;) | Out-Null<br>$objForm = New-Object System.Windows.Forms.OpenFileDialog<br>$objForm.InitialDirectory = $Directory<br>$objForm.Filter = $Filter<br>$objForm.Title = $Title<br>$Show = $objForm.ShowDialog()<br>If ($Show -eq &ldquo;OK&rdquo;)<br>{<br>Return $objForm.FileName<br>}<br>Else<br>{<br>Write-Error &ldquo;Operation cancelled by user.&rdquo;<br>}<br>}<br>[/ps]</p><p>I then call the function like this:</p><p>[ps]<br>$log = Select-FileDialog -Title &ldquo;Select an Apache logfile&rdquo;<br>[/ps]</p><p>A little Regex magic runs through the logfiles, and captures the different elements into different named capturing groups, in this step, I expand all referrer hostnames, and put them into the $urls variable</p><p>[ps]<br>$urls = Select-String &lsquo;^(?&lt;client>\S+)\s+(?&lt;auth>\S+\s+\S+)\s+[(?&lt;datetime>[^]]+)]\s+&rdquo;(?:GET|POST|HEAD) (?&lt;file>[^ ?"]+)??(?&lt;parameters>[^ ?"]+)? HTTP/[0-9.]+&rdquo;\s+(?&lt;status>[0-9]+)\s+(?&lt;size>[-0-9]+)\s+&rdquo;(?&lt;referrer>[^&rdquo;]*)"\s+&rdquo;(?&lt;useragent>[^&rdquo;]*)&ldquo;$&rsquo; $log |<br>Select -Expand Matches | Foreach { $_.Groups[&ldquo;referrer&rdquo;].value }<br>[/ps]<br>I modified a script by Joel Bennet, to get a little statistics as well, since there can be 1000&rsquo;s of hostnames, I have selected only to output top 15 by default (using the $URLCount variable.</p><p>[ps]<br>$urls | group | ForEach -begin { $total = 0 } `<br>-process { $total += $_.Count; $_ } |Sort Count | Select Count, Name |<br>Add-Member ScriptProperty Percent { &ldquo;{0,15:0.00}%&rdquo; -f (100*$this.Count/$Total) } -Passthru | select -Last $URLCount<br>[/ps]</p><p>Then I loop through all the hostnames, and extract the base domain name, using regex again. (Here I choose to ignore all traffic from my own domain name Xipher.dk, and I choose only to look for referral domains, that have generated 100 hits or more</p><p>[ps]<br>$hosts = $urls | Select-String &lsquo;\b[a-z][a-z0-9+-.]*://([a-z0-9-._~%!$&()*+,;=]+@)?(?&lt;host>[a-z0-9-._~%]+|[[a-z0-9-._~%!$&()*+,;=:]+])&rsquo; |<br>Select -Expand Matches | Foreach { $_.Groups[&ldquo;host&rdquo;].value } | group | sort count | where {($_.name -notlike &ldquo;*xipher.dk*") -and ($_.Count -gt 100)} |<br>ForEach -begin { $total = 0 } `<br>-process { $total += $_.Count; $_ } | Sort Count | Select Count, Name |<br>Add-Member ScriptProperty Percent { &ldquo;{0,10:0.00}%&rdquo; -f (100*$this.Count/$Total) } -Passthru<br>[/ps]</p><p>The script expects to find a .htaccess file in c:\temp containing at least the following two lines:</p><p>RewriteEngine On<br>RewriteRule (.*) http://%{REMOTE_ADDR}/$ [R=301,L]</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=/posts/2012-01-25-checking-site-sizes-in-sharepoint-2007-and-2010/><span class=button__icon>←</span>
<span class=button__text>Checking Site sizes in SharePoint 2007 and 2010</span></a></span>
<span class="button next"><a href=/posts/2011-09-18-finally-got-confirmation-grown-men-do-cry-when-their-bitlocker-encrypted-ssd-disk-dies/><span class=button__text>Finally got confirmation.. Grown men do cry when their Bitlocker encrypted SSD disk dies ;(</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2020 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/assets/main.js></script><script src=/assets/prism.js></script></div></body></html>